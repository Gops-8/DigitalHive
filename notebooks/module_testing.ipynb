{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gops/work/market_analysis/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/gops/work/market_analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "# import dateto\n",
    "# Add the 'src' directory to the Python path\n",
    "# src_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "# if src_path not in sys.path:\n",
    "#     sys.path.append(src_path)\n",
    "\n",
    "# Test importing a module from 'src'\n",
    "from src.core.scraper import WebScraper\n",
    "from src.core.content_analyzer import ContentAnalyzer\n",
    "from src.core.data_processer import DataProcessor\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor = DataProcessor()\n",
    "scrapper=WebScraper()\n",
    "analyzer = ContentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'admin': 'newpassword'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "AUTH_CONFIG_URL = 'https://raw.githubusercontent.com/gops-8/auth-config/main/config.json'\n",
    "response = requests.get(AUTH_CONFIG_URL)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=data_processor.read_excel_to_url(file_path='/home/gops/work/market_analysis/input/test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for url in urls:\n",
    "   \n",
    "#     clean_url=data_processor.clean_url(url)\n",
    "#     print(url,clean_url)\n",
    "#     try:\n",
    "#          data=scrapper.scrape_website(clean_url)\n",
    "#     except:\n",
    "#         print(f'Error in scrapping : {clean_url}')\n",
    "#         continue\n",
    "#     print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21 URLs\n",
      "\n",
      "Processing [1/21]: url\n",
      "✗ Scraping error: name 'web_scraper' is not defined\n",
      "\n",
      "Processing [2/21]: sanjosebengalcats.com\n",
      "✗ Scraping error: name 'web_scraper' is not defined\n",
      "\n",
      "Processing [3/21]: peachmarketplace.com\n",
      "✗ Scraping error: name 'web_scraper' is not defined\n",
      "\n",
      "Processing [4/21]: peachmcintyre.com\n",
      "✗ Scraping error: name 'web_scraper' is not defined\n",
      "\n",
      "Processing [5/21]: peachmedical.com\n",
      "✗ Scraping error: name 'web_scraper' is not defined\n",
      "\n",
      "Processing [6/21]: peachmeleggings.com\n",
      "✗ Scraping error: name 'web_scraper' is not defined\n",
      "\n",
      "Processing [7/21]: peachmode.com\n",
      "✗ Scraping error: name 'web_scraper' is not defined\n",
      "\n",
      "Processing [8/21]: peachmodern.com\n",
      "✗ Scraping error: name 'web_scraper' is not defined\n",
      "\n",
      "Processing [9/21]: peachmountain.com\n",
      "✗ Scraping error: name 'web_scraper' is not defined\n",
      "\n",
      "Processing [10/21]: peachofmind.com\n",
      "✗ Scraping error: name 'web_scraper' is not defined\n",
      "\n",
      "Processing [11/21]: peachonaleash.com\n",
      "✗ Scraping error: name 'web_scraper' is not defined\n",
      "\n",
      "Processing [12/21]: peachorchardapartments.com\n",
      "✗ Scraping error: name 'web_scraper' is not defined\n",
      "\n",
      "Processing [13/21]: peachpaperdesign.com\n",
      "✗ Scraping error: name 'web_scraper' is not defined\n",
      "\n",
      "Processing [14/21]: peachperfectskincare.com\n",
      "✗ Scraping error: name 'web_scraper' is not defined\n",
      "\n",
      "Processing [15/21]: peachpitgym.com\n",
      "✗ Scraping error: name 'web_scraper' is not defined\n",
      "\n",
      "Processing [16/21]: peachpitmusic.com\n",
      "✗ Scraping error: name 'web_scraper' is not defined\n",
      "\n",
      "Processing [17/21]: peachpitpotteryshop.com\n",
      "✗ Scraping error: name 'web_scraper' is not defined\n",
      "\n",
      "Processing [18/21]: peach-pump.com\n",
      "✗ Scraping error: name 'web_scraper' is not defined\n",
      "\n",
      "Processing [19/21]: peachstatebasketball.com\n",
      "✗ Scraping error: name 'web_scraper' is not defined\n",
      "\n",
      "Processing [20/21]: peachstatecandlesupply.com\n",
      "✗ Scraping error: name 'web_scraper' is not defined\n",
      "\n",
      "Processing [21/21]: peachstateco.com\n",
      "✗ Scraping error: name 'web_scraper' is not defined\n",
      "\n",
      "Summary:\n",
      "Total URLs processed: 21\n",
      "Successful: 0\n",
      "Empty responses: 0\n",
      "Failed: 21\n",
      "\n",
      "Results saved to: scraping_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Read URLs into DataFrame\n",
    "    urls = data_processor.read_excel_to_url('/home/gops/work/market_analysis/input/test.xlsx')\n",
    "    print(f\"Found {len(urls)} URLs\")\n",
    "    \n",
    "    # Sample 50 URLs from DataFrame\n",
    "    # urls = urls[:10]\n",
    "    \n",
    "    # Initialize output dictionary\n",
    "    output = {}\n",
    "    \n",
    "    for i, url in enumerate(urls, 1):\n",
    "        try:\n",
    "            # Skip empty or invalid URLs\n",
    "            if not url or pd.isna(url):\n",
    "                print(f\"Skipping empty URL at position {i}\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\nProcessing [{i}/{len(urls)}]: {url}\")\n",
    "            \n",
    "            # Clean URL\n",
    "            try:\n",
    "                clean_url = data_processor.clean_url(url)\n",
    "                if not clean_url:\n",
    "                    raise ValueError(\"URL cleaning resulted in empty URL\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error cleaning URL {url}: {str(e)}\")\n",
    "                output[url] = {\n",
    "                    'cleaned_url': None,\n",
    "                    'status': 'error',\n",
    "                    'data': None,\n",
    "                    'error': f\"URL cleaning error: {str(e)}\",\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                continue\n",
    "            \n",
    "            # Initialize dictionary for this URL\n",
    "            output[url] = {\n",
    "                'cleaned_url': clean_url,\n",
    "                'status': 'pending',\n",
    "                'data': None,\n",
    "                'error': None,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            # Add scheme if missing\n",
    "            if not clean_url.startswith(('http://', 'https://')):\n",
    "                clean_url = 'https://' + clean_url\n",
    "            \n",
    "            # Scrape website\n",
    "            try:\n",
    "                scraped_data = web_scraper.scrape_website(clean_url)\n",
    "                \n",
    "                # Check if scraping was successful\n",
    "                if scraped_data.get('content') or (scraped_data.get('metadata', {}).get('title') != 'Loading...' \n",
    "                    and scraped_data.get('metadata', {}).get('title') is not None):\n",
    "                    output[url].update({\n",
    "                        'status': 'success',\n",
    "                        'data': {\n",
    "                            'title': scraped_data.get('metadata', {}).get('title', ''),\n",
    "                            'meta_description': scraped_data.get('metadata', {}).get('meta_description', ''),\n",
    "                            'content': scraped_data.get('content', '')\n",
    "                        }\n",
    "                    })\n",
    "                    print(f\"✓ Success - {clean_url}\")\n",
    "                else:\n",
    "                    output[url].update({\n",
    "                        'status': 'empty',\n",
    "                        'error': 'No content scraped'\n",
    "                    })\n",
    "                    print(f\"✗ Empty response - {clean_url}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                output[url].update({\n",
    "                    'status': 'error',\n",
    "                    'error': f\"Scraping error: {str(e)}\"\n",
    "                })\n",
    "                print(f\"✗ Scraping error: {str(e)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            if url in output:\n",
    "                output[url].update({\n",
    "                    'status': 'error',\n",
    "                    'error': f\"Processing error: {str(e)}\"\n",
    "                })\n",
    "            else:\n",
    "                output[url] = {\n",
    "                    'cleaned_url': None,\n",
    "                    'status': 'error',\n",
    "                    'data': None,\n",
    "                    'error': f\"Processing error: {str(e)}\",\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "            print(f\"✗ Processing error: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Print summary\n",
    "    success_count = len([v for v in output.values() if v['status'] == 'success'])\n",
    "    empty_count = len([v for v in output.values() if v['status'] == 'empty'])\n",
    "    error_count = len([v for v in output.values() if v['status'] == 'error'])\n",
    "    \n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"Total URLs processed: {len(output)}\")\n",
    "    print(f\"Successful: {success_count}\")\n",
    "    print(f\"Empty responses: {empty_count}\")\n",
    "    print(f\"Failed: {error_count}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame.from_dict(output, orient='index')\n",
    "    results_df.reset_index(inplace=True)\n",
    "    results_df.rename(columns={'index': 'original_url'}, inplace=True)\n",
    "    \n",
    "    # Save results\n",
    "    output_file = f\"scraping_results.xlsx\"\n",
    "    results_df.to_excel(output_file, index=False)\n",
    "    print(f\"\\nResults saved to: {output_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Main process error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
